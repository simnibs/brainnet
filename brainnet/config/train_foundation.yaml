PROJECT_NAME: &PROJECT BrainNet # same as wandb project name!

# DATASET     MODALITIES
# ----        ----------
# ADNI        T1
# ADNI3       T1, FLAIR
# ADHD200     T1
# AIBL        T1, T2, FLAIR
# HCP         T1, T2
# OASIS3      CT

# HEADRECO    T1, T2, CT, ...


# Logging using Weights & Biases (https://wandb.ai)
wandb:
  enable: false
  project_name: *PROJECT
  # entity: null  # username/team name to send data to
  tags: null
  name: null      # name used to identify run in UI
  # dir: null      # sub dir of results dir


device:
  model: "cuda:0"       # the model we are training
  synthesizer: "cpu" # the code synthesizing the images passed to the task network

# DATASET
# =====================================

dataset:
  dir: !Path /mnt/scratch/personal/jesperdn/datasets

  train: [HCP, WH2015] #[ADNI, OASIS3]
  test: [] #[HCP]

  # which optional images to use for each dataset. Images should be valid
  optional_images:
    HCP: []
    WH2015: [PD]

  # if a synthetic image is not generated then use one of these real images instead
  # (only relevant when probability of generating a synthetic contrast is < 1.0)
  alternative_synth:
    HCP: [norm]
    WH2015: [norm, PD]

  split:
    rng_seed: null # seed for reproducible dataset splits (null = no seed)
    splits:
      train: 0.8
      validation: 0.2

  kwargs:
    # Initial surface resolution
    surface_resolution: 0
    surface_hemi: random
    # These images should be present for all datasets
    #   constants.filenames.default_images
    default_images: [generation, segmentation, norm]

  dataloader:
    batch_size: 1
    num_workers: 2
    prefetch_factor: 2 # this seems to be PER dataset


synthesizer:
  config: null        # config file to use. If null, use default from BrainSynth
  module: Synthesizer # Currently, there is only one synthesizer so...

# RESULTS
# =====================================

# results are stored in resultsdir/PROJECT_NAME/...
results:
  dir: !Path /mnt/scratch/personal/jesperdn/results     # PROJECT_NAME will be appended
  # checkpoints: checkpoints # store checkpoints in this subdir of dir/project/

# NOTE paths should be relative to the directory of the current yaml file

# MODEL
# =====================================

# > contents of this node is passed to BrainNet
model: !include models/foundation.yaml

# LOSS
# =====================================

# Specify the loss
# > contents of this node is passed to Criterion
loss: !include loss/foundation.yaml

# if input is not explicitly specified, then the name of the task is used
# &INIT_DK {softmax: true, to_onehot_y: true}

# TRAINING PARAMETERS
# =====================================

epoch:
  resume_from_checkpoint: null # e.g., 100
  max_allowed_epochs: 1000
  steps_per_epoch: 100
  validate_every: 20
  save_state_every: 20

  # resume_optim: true
  # resume_lr_scheduler: true

optimizer:
  name: AdamW
  kwargs:
    lr: 1.0e-4

convergence:
  minimum_lr: &MIN_LR 1.0e-8

auto_schedulers:
  # ChainedSchedulers are called on validation loss, thus "epochs" refers to the number
  # of validation evaluations!
  # lr:
  #   ReduceLROnPlateau:
  #     patience: 20      # accepted number of "epochs" with no improvement
  #     factor: 0.5       # LR reduction factor
  #     threshold: 1.0e-3   # threshold for relative loss decrease
  #     min_lr: *MIN_LR      # lower bound on LR
  #     verbose: true

# DEEPSURFER uses three phases with different weighting of these losses:
#           matched chamfer Hinge
# stage 1 : 1       1       100     end epoch 400
# stage 2 : 0.1     1       10      end epoch 800
# stage 3 : 0.0001  1       0.5/0.2 (white/pial)

# At the specified epoch, update the entry
manual_schedulers:
  # loss_weights:
  #   "400":
  #     surface:
  #       white:
  #         MatchedDistanceLoss: 0.1
  #         SymmetricChamferLoss: 1.0
  #         SymmetricCurvatureLoss: 10.0
  #         EdgeLengthVarianceLoss: 1.0
  #       pial:
  #         MatchedDistanceLoss: 0.1
  #         SymmetricChamferLoss: 1.0
  #         SymmetricCurvatureLoss: 10.0
  #         EdgeLengthVarianceLoss: 1.0
  #   "800":
  #     surface:
  #       white:
  #         MatchedDistanceLoss: 0.0001
  #         SymmetricChamferLoss: 1.0
  #         SymmetricCurvatureLoss: 0.5
  #         EdgeLengthVarianceLoss: 1.0
  #       pial:
  #         MatchedDistanceLoss: 0.0001
  #         SymmetricChamferLoss: 1.0
  #         SymmetricCurvatureLoss: 0.2
  #         EdgeLengthVarianceLoss: 1.0
  # tasks:
  #   "1000":
  #     surface:
  #       module_kwargs:
  #         prediction_res: 5
  #   "2000":
  #     surface:
  #       module_kwargs:
  #         prediction_res: 6
