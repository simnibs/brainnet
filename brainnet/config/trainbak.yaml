PROJECT_NAME: &PROJECT BrainNetEdgeNet # same as wandb project name!
RUN_NAME: &RUN NewUNet

# ============================================================================
# WANDB
# ============================================================================

# Logging using Weights & Biases (https://wandb.ai)
wandb:
  enable: true
  project_name: *PROJECT
  name: *RUN      # name used to identify run in UI
  resume: true
  tags: null
  # entity: null  # username/team name to send data to
  # dir: null      # sub dir of results dir


# ============================================================================
# DEVICES
# ============================================================================

device:
  model: "cuda:0"    # the model we are training
  synthesizer: "cuda:0" # the code synthesizing the images passed to the task network

# ============================================================================
# DATASET
# ============================================================================

dataset:
  dir: !Path /mnt/scratch/personal/jesperdn/datasets
  datasets: [HCP, OASIS3]




  # will look for subjects_{dataset}_(train|validation|...).txt in dir
  train:

    # which optional images to use for each dataset. Images should be valid
    optional_images:
      HCP: [] #[T1]
      OASIS3: [] #[T1] # e.g., [PD]

    # if a synthetic image is not generated then use one of these real images instead
    # (only relevant when probability of generating a synthetic contrast is < 1.0 or
    # disable_synth is true)
    alternative_synth:
      HCP: [norm] #[norm, T1]
      OASIS3: [norm] #[norm, T1] # e.g., [norm, PD]

    # split:
    #   rng_seed: 0 # seed for reproducible dataset splits (null = no seed)
    #   splits:
    #     train: 0.9
    #     validation: 0.1
    #   # splits:
    #   #   train: 1.0

    kwargs:
      surface_resolution: &SURF_RES 4 # 0 - 6
      surface_hemi: both # lh
      # These images should be present for all datasets
      #   constants.filenames.default_images
      default_images: [generation, segmentation, norm]

    dataloader:
      batch_size: 1
      num_workers: 8
      prefetch_factor: 4 # this seems to be PER dataset

  validation:
    # Dataset kwargs
    dataset_kwargs:
      surface_resolution: &SURF_RES
      surface_hemi: both
      default_images: [generation, segmentation, norm]
      optional_images:
        HCP: [T1]
        OASIS3: [T1] # e.g., [PD]
      alternative_synth:
        HCP: [norm, T1]
        OASIS3: [norm, T1] # e.g., [norm, PD]

    synthesizer_kwargs:

    dataloader_kwargs:
      batch_size: 1
      num_workers: 8
      prefetch_factor: 4 # this seems to be PER dataset
      # pin_memory: true


# ============================================================================
# SYNTHESIZER
# ============================================================================

synthesizer:
  config: null        # config file to use. If null, use default from BrainSynth
  module: Synthesizer # Currently, there is only one synthesizer so...


# ============================================================================
# RESULTS
# ============================================================================

# results are stored in results_dir/PROJECT_NAME/RUN_NAME/
results:
  dir: !Path /mnt/scratch/personal/jesperdn/results # PROJECT_NAME will be appended

# NOTE when including yaml files, paths should be relative to the directory of
# the current yaml file


# ============================================================================
# MODEL
# ============================================================================

# > contents of this node is passed to BrainNet
model: !include models/foundation.yaml

# ============================================================================
# LOSS
# ============================================================================

# Specify the loss
# > contents of this node is passed to Criterion
loss: !include loss/foundation_0000.yaml


# ============================================================================
# TRAINING PARAMETERS
# ============================================================================

epoch:
  resume_from_checkpoint: 600
  max_allowed_epochs: 1000
  steps_per_epoch: &STEPS 100
  steps_per_validation: 50
  validate_every: 10
  save_state_every: 20
  load_optimizer_checkpoint: true # true

# estimate inter-task affinities
estimate_ITA: false # true

# SGD     1e-3 - 1e-8
# AdamW   1e-2 - 1e-8

optimizer:
  name: Adam
  kwargs:
    lr: 1.0e-4 # the default lr

  # body: BrainNet.body.[]
  # heads:  BrainNet.heads.[]
  # lr_parameter_groups:
    # body: 1.0e-4
    # heads:
      # surface: 1.0e-4
      # segmentation: 1.0e-3

# optimizer:
#   name: SGD
#   kwargs:
#     lr: 1.0e-8
#     momentum: 0.9

# 100 & 1
# 10 & 10
# lr_scheduler:
  # model: OneCycleLR
  # kwargs:
  #   # max_lr: 1.0e-6 # set by parameters from optimizer section
  #   # epochs: 200
  #   # steps_per_epoch: *STEPS
  #   total_steps: 200
  #   # pct_start=0.3
  #   div_factor: 100   # 100     # init_lr  = max_lr / div_factor
  #   final_div_factor: 1 # 1  # final_lr = in100it_lr / final_div_factor = max_lr / (div_factor * final_div_factor)
  #   # three_phase: True

  # model: CyclicLR
  # kwargs:
  #   # base_lr: 1.0e-6 # set by parameters from optimizer section
  #   # max_lr: 1.0e-4 # set a multiplication factor instead!
  #   step_size_up: 100 # = step_size_down
  #   cycle_momentum: false
  #   max_lr_factor: 5 # multiplication factor: max_lr = base_lr * max_lr_factor

#   model: CosineAnnealingLR
#   kwargs:
#     T_max: 200000 # 100 * 200
#     eta_min: 1.0e-3

  # model: LinearLR
  # kwargs:
  #   start_factor: 0.1
  #   end_factor: 1.0
  #   total_iters: 50 # epochs !

convergence:
  minimum_lr: &MIN_LR 1.0e-10

auto_schedulers:
  # ChainedSchedulers are called on validation loss, thus "epochs" refers to the number
  # of validation evaluations!
  # lr:
  #   ReduceLROnPlateau:
  #     patience: 20      # accepted number of "epochs" with no improvement
  #     factor: 0.5       # LR reduction factor
  #     threshold: 1.0e-3   # threshold for relative loss decrease
  #     min_lr: *MIN_LR      # lower bound on LR
  #     verbose: true



# LossWeightScheduler
# action: MultiplyEvery
# frequency: 50
# factor: 0.9



# At the specified epoch, update the entry
manual_schedulers:
  # "200":

  # loss:
  #   loss_weights:
  #     white:
  #       hinge: 50



  # # "400":
  # surface:
  #   module:
  #     name: SurfaceModule
  #     # kwargs:
  #     #   prediction_res: 4 # available resolutions: 0 - 6
  #     run_kwargs:
  #       return_pial: true

  # loss:
  #   loss_weights:
  #     white:
  #       hinge: 50

  # "800":
  #   loss: !include loss/foundation_0800.yaml
  #   tasks: # !include models/foundation_tasks_0800.yaml
  #     surface:
  #       module:
  #         name: SurfaceModule
  #         kwargs:
  #           prediction_res: 5

  # "2000":
  #     surface:
  #       module:
  #         name: SurfaceModule
  #         kwargs:
  #           prediction_res: 6
